#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
TWSE æ—¥ç·šè³‡æ–™æŠ“å–å™¨ï¼ˆå«å¿«å– / äº¤æ˜“æ—¥è¡Œäº‹æ›† / é‡è©¦ / æ‰¹æ¬¡å…¥åº« / CLIï¼‰
"""

import os
import sys
import re
import time
import json
import math
import sqlite3
import argparse
import logging
from logging.handlers import RotatingFileHandler
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import List, Optional, Iterable, Tuple

import pandas as pd
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ---------------------------
# è·¯å¾‘èˆ‡å¸¸æ•¸
# ---------------------------

# ---------------------------
# è·¯å¾‘èˆ‡å¸¸æ•¸
# ---------------------------

def get_base_dir():
    if getattr(sys, 'frozen', False):
        return Path(sys.executable).resolve().parent
    else:
        return Path(__file__).resolve().parent

BASE_DIR = get_base_dir()
DEFAULT_DATA_DIR = BASE_DIR / "data"
DEFAULT_DB_PATH = DEFAULT_DATA_DIR / "twse_data.db"
CALENDAR_CACHE_TEMPLATE = "calendar_{year}.json"
TWSE_MI_INDEX = "https://www.twse.com.tw/exchangeReport/MI_INDEX"
TWSE_CALENDAR_HTML = "https://www.twse.com.tw/holidaySchedule/holidaySchedule?queryYear={roc}&response=html"

# (B) å¿«å–æ¬„ä½å›ºå®šæ¸…å–®
CACHE_COLUMNS = ["æ—¥æœŸ", "ä»£è™Ÿ", "åç¨±", "é–‹ç›¤", "æœ€é«˜", "æœ€ä½", "æ”¶ç›¤", "æˆäº¤é‡‘é¡", "è³‡æ–™ä¾†æº", "ä¸‹è¼‰æ™‚é–“"]

# ---------------------------
# Logging
# ---------------------------

def setup_logging(log_level: str, data_dir: Path) -> None:
    data_dir.mkdir(parents=True, exist_ok=True)
    level = getattr(logging, log_level.upper(), logging.INFO)

    logger = logging.getLogger()
    logger.setLevel(level)

    fmt = logging.Formatter(
        "%(asctime)s | %(levelname)s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(level)
    ch.setFormatter(fmt)
    logger.addHandler(ch)

    fh = RotatingFileHandler(data_dir / "reader.log", maxBytes=2_000_000, backupCount=3, encoding="utf-8")
    fh.setLevel(level)
    fh.setFormatter(fmt)
    logger.addHandler(fh)

# ---------------------------
# Requests Session with Retry
# ---------------------------

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def make_session(max_retries: int = 3, backoff: float = 0.5, timeout: int = 12, verify: bool = True) -> requests.Session:
    s = requests.Session()
    retry = Retry(
        total=max_retries,
        read=max_retries,
        connect=max_retries,
        status=max_retries,
        backoff_factor=backoff,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=("GET",),
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    s.headers.update({"User-Agent": "Mozilla/5.0"})
    s.request = _with_timeout_and_verify(s.request, timeout, verify)  # inject default timeout & verify
    return s

def _with_timeout_and_verify(request_func, timeout: int, verify: bool):
    def wrapper(method, url, **kwargs):
        if "timeout" not in kwargs:
            kwargs["timeout"] = timeout
        if "verify" not in kwargs:
            kwargs["verify"] = verify
        return request_func(method, url, **kwargs)
    return wrapper

# ---------------------------
# DB
# ---------------------------

def init_db(db_path: Path) -> None:
    logging.info("åˆå§‹åŒ– SQLiteï¼š%s", db_path)
    db_path.parent.mkdir(parents=True, exist_ok=True)
    with sqlite3.connect(db_path) as conn:
        cur = conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS stock_prices (
                æ—¥æœŸ TEXT,
                ä»£è™Ÿ TEXT,
                åç¨± TEXT,
                é–‹ç›¤ REAL,
                æœ€é«˜ REAL,
                æœ€ä½ REAL,
                æ”¶ç›¤ REAL,
                æˆäº¤é‡‘é¡ INTEGER CHECK(æˆäº¤é‡‘é¡ >= 0),
                è³‡æ–™ä¾†æº TEXT,
                ä¸‹è¼‰æ™‚é–“ TEXT,
                PRIMARY KEY (æ—¥æœŸ, ä»£è™Ÿ)
            )
            """
        )
        # (A) ç´¢å¼•ï¼‹PRAGMA æå‡æ‰¹æ¬¡æ•ˆèƒ½
        info = {row[1]: (row[2] or "").upper() for row in cur.execute("PRAGMA table_info(stock_prices)")}
        if info.get("æˆäº¤é‡‘é¡") and info["æˆäº¤é‡‘é¡"] not in {"INTEGER", "INT"}:
            logging.warning("è³‡æ–™è¡¨ stock_prices çš„ã€Œæˆäº¤é‡‘é¡ã€æ¬„ä½å‹åˆ¥ç‚º %sï¼Œå»ºè­°èª¿æ•´ç‚º INTEGERã€‚", info["æˆäº¤é‡‘é¡"])
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_stock_prices_code_date
            ON stock_prices(ä»£è™Ÿ, æ—¥æœŸ)
        """)
        cur.execute("PRAGMA journal_mode=WAL;")
        cur.execute("PRAGMA synchronous=NORMAL;")
        conn.commit()

def bulk_upsert(db_path: Path, rows: Iterable[Tuple]) -> int:
    rows = list(rows)
    if not rows:
        return 0
    with sqlite3.connect(db_path) as conn:
        cur = conn.cursor()
        cur.executemany(
            """
            INSERT INTO stock_prices
            (æ—¥æœŸ, ä»£è™Ÿ, åç¨±, é–‹ç›¤, æœ€é«˜, æœ€ä½, æ”¶ç›¤, æˆäº¤é‡‘é¡, è³‡æ–™ä¾†æº, ä¸‹è¼‰æ™‚é–“)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(æ—¥æœŸ, ä»£è™Ÿ) DO UPDATE SET
                åç¨±=excluded.åç¨±,
                é–‹ç›¤=excluded.é–‹ç›¤,
                æœ€é«˜=excluded.æœ€é«˜,
                æœ€ä½=excluded.æœ€ä½,
                æ”¶ç›¤=excluded.æ”¶ç›¤,
                æˆäº¤é‡‘é¡=excluded.æˆäº¤é‡‘é¡,
                è³‡æ–™ä¾†æº=excluded.è³‡æ–™ä¾†æº,
                ä¸‹è¼‰æ™‚é–“=excluded.ä¸‹è¼‰æ™‚é–“
            """,
            rows,
        )
        conn.commit()
        return cur.rowcount

# ---------------------------
# äº¤æ˜“æ—¥è¡Œäº‹æ›†
# ---------------------------

def roc_year(dt: date) -> int:
    return dt.year - 1911

def _calendar_cache_path(data_dir: Path, year: int) -> Path:
    return Path(data_dir) / CALENDAR_CACHE_TEMPLATE.format(year=year)


def _load_calendar_cache(data_dir: Path, year: int) -> Optional[Tuple[set, set]]:
    cache_path = _calendar_cache_path(data_dir, year)
    if not cache_path.exists():
        return None
    try:
        payload = json.loads(cache_path.read_text(encoding="utf-8"))
        holidays = {date.fromisoformat(d) for d in payload.get("holidays", [])}
        makeups = {date.fromisoformat(d) for d in payload.get("makeups", [])}
        logging.info("ä½¿ç”¨å¿«å–è¡Œäº‹æ›†ï¼š%s å¹´", year)
        return holidays, makeups
    except Exception as exc:
        logging.warning("è¡Œäº‹æ›†å¿«å–æª”è®€å–å¤±æ•—ï¼š%s", exc)
        return None


def _store_calendar_cache(data_dir: Path, year: int, holidays: set, makeups: set) -> None:
    cache_path = _calendar_cache_path(data_dir, year)
    try:
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        cache_path.write_text(
            json.dumps(
                {
                    "year": year,
                    "holidays": sorted(d.isoformat() for d in holidays),
                    "makeups": sorted(d.isoformat() for d in makeups),
                    "cached_at": datetime.utcnow().isoformat() + "Z",
                },
                ensure_ascii=False,
                indent=2,
            ),
            encoding="utf-8",
        )
    except Exception as exc:
        logging.warning("ç„¡æ³•å¯«å…¥ %s å¹´è¡Œäº‹æ›†å¿«å–æª”ï¼š%s", year, exc)


def try_fetch_holidays_and_makeups(session: requests.Session, year: int, data_dir: Path, refresh: bool=False) -> Tuple[set, set]:
    """
    å¾ TWSE é–‹ä¼‘å¸‚é é¢è§£æï¼š
    - holidays: æ”¾å‡æ—¥ï¼ˆå¸‚å ´ä¼‘å¸‚ï¼‰
    - makeups: èª¿æ•´ä¸Šç­æ—¥ï¼ˆå¯èƒ½ç‚ºå…­/æ—¥è½‰ä¸Šç­æ—¥ï¼‰
    è‹¥è§£æå¤±æ•—ï¼Œå›å‚³ç©ºé›†åˆã€‚
    """
    if not refresh:
        cached = _load_calendar_cache(data_dir, year)
        if cached is not None:
            return cached
    url = TWSE_CALENDAR_HTML.format(roc=roc_year(date(year, 1, 1)))
    logging.info("å˜—è©¦å–å¾— %s ä¹‹é–‹ä¼‘å¸‚è³‡è¨Šï¼š%s", year, url)
    try:
        r = session.get(url)
        r.raise_for_status()
        tables = pd.read_html(r.text)
    except Exception as e:
        logging.warning("è§£æé–‹ä¼‘å¸‚é é¢å¤±æ•—ï¼š%s", e)
        return set(), set()

    holidays, makeups = set(), set()
    date_pattern = re.compile(r"\d{3,4}/\d{1,2}/\d{1,2}")  # 114/1/1 æˆ– 2025/1/1

    def _to_gregorian(dstr: str) -> Optional[date]:
        dstr = str(dstr).strip()
        m = date_pattern.search(dstr)
        if not m:
            return None
        token = m.group(0)
        parts = token.split("/")
        if len(parts[0]) <= 3:  # ROC å¹´
            y = int(parts[0]) + 1911
        else:
            y = int(parts[0])
        try:
            return date(y, int(parts[1]), int(parts[2]))
        except ValueError:
            return None

    for df in tables:
        cols = "".join(map(str, df.columns))
        if not any(k in cols for k in ("æ—¥", "æœŸ")):
            continue
        for _, row in df.iterrows():
            row_text = " ".join(map(lambda x: str(x), row.values))
            d = _to_gregorian(row_text)
            if not d or d.year != year:
                continue
            txt = row_text
            if any(k in txt for k in ("ä¼‘å¸‚", "æ”¾å‡", "åœæ­¢äº¤æ˜“", "è£œå‡", "ä¸­ç§‹", "æ˜¥ç¯€", "åœ‹æ…¶", "é€£å‡", "é™¤å¤•")):
                holidays.add(d)
            if any(k in txt for k in ("è£œè¡Œä¸Šç­", "èª¿æ•´ä¸Šç­", "è£œç­")):
                makeups.add(d)

    logging.info("è§£æåˆ° %d å€‹ä¼‘å¸‚æ—¥ã€%d å€‹è£œç­æ—¥ï¼ˆ%sï¼‰", len(holidays), len(makeups), year)
    _store_calendar_cache(data_dir, year, holidays, makeups)
    return holidays, makeups

def build_trading_days(session: requests.Session, start: date, end: date, data_dir: Path, refresh_calendar: bool=False) -> List[date]:
    """
    å„ªå…ˆä½¿ç”¨å®˜æ–¹é–‹ä¼‘å¸‚é é¢æ¨å°äº¤æ˜“æ—¥ï¼š
      äº¤æ˜“æ—¥ = æ‰€æœ‰å¹³æ—¥(ä¸€~äº”) - ä¼‘å¸‚æ—¥ + è£œç­æ—¥(å¦‚è½åœ¨é€±æœ«)
    è‹¥å¤±æ•—ï¼Œé€€å›ä¿å®ˆæ¨¡å¼ï¼šå¹³æ—¥(ä¸€~äº”)ï¼Œä¸”å¾ŒçºŒä»¥ API æœ‰è³‡æ–™ç‚ºæº–ã€‚
    """
    # (A) ä¿®æ­£ï¼šæ¶µè“‹æ‰€æœ‰è·¨å¹´çš„å¹´ä»½
    years = list(range(start.year, end.year + 1))
    holidays_all, makeups_all = set(), set()
    for y in years:
        h, m = try_fetch_holidays_and_makeups(session, y, data_dir, refresh=refresh_calendar)
        holidays_all |= h
        makeups_all |= m

    days: List[date] = []
    cur = start
    while cur <= end:
        is_weekday = 1 <= cur.isoweekday() <= 5
        is_makeup = cur in makeups_all  # é€±æœ«è£œç­
        if (is_weekday and cur not in holidays_all) or is_makeup:
            days.append(cur)
        cur += timedelta(days=1)

    if not days:
        logging.warning("è¡Œäº‹æ›†è§£æç‚ºç©ºï¼Œæ”¹ç”¨å¹³æ—¥(ä¸€~äº”)ä¿å®ˆæ¨¡å¼ã€‚")
        cur = start
        days = []
        while cur <= end:
            if 1 <= cur.isoweekday() <= 5:
                days.append(cur)
            cur += timedelta(days=1)

    logging.info("äº¤æ˜“æ—¥ç¯„åœï¼š%s ~ %sï¼Œå…± %d å¤©ï¼ˆå«è£œç­æ—¥ï¼‰", start, end, len(days))
    return days

# ---------------------------
# ä¸‹è¼‰ + å¿«å–
# ---------------------------

def cache_path_for(day: date, data_dir: Path, out_format: str) -> Path:
    ext = ".csv.gz" if out_format == "csv.gz" else ".csv"
    return data_dir / f"ohlcv_{day.strftime('%Y%m%d')}{ext}"


def cache_candidates_for(day: date, data_dir: Path, preferred_format: str) -> List[Path]:
    primary = cache_path_for(day, data_dir, preferred_format)
    alt_format = "csv" if preferred_format == "csv.gz" else "csv.gz"
    candidates = [primary]
    alt_path = cache_path_for(day, data_dir, alt_format)
    if alt_path != primary:
        candidates.append(alt_path)
    return candidates

def fetch_one_day(session: requests.Session, day: date, data_dir: Path, force: bool=False,
                  cache_format: str="csv", from_cache_only: bool=False) -> Optional[pd.DataFrame]:
    """
    ä¸‹è¼‰å–®æ—¥ TWSE ALL å ±è¡¨ï¼Œå„²å­˜ç‚º CSV å¿«å–ä¸¦å›å‚³æ¸…ç†å¾Œçš„ DataFrameã€‚
    å›å‚³ None è¡¨ç¤ºè©²æ—¥æ²’æœ‰å¯ç”¨è³‡æ–™æˆ–å¤±æ•—ã€‚
    """
    data_dir.mkdir(parents=True, exist_ok=True)
    if from_cache_only:
        force = False
    candidates = cache_candidates_for(day, data_dir, cache_format)
    target_path = candidates[0]

    cache_hit_path: Optional[Path] = None
    if not force:
        for candidate in candidates:
            if candidate.exists():
                cache_hit_path = candidate
                break

    # (B) è®€å¿«å–æ™‚å›ºå®šæ¬„ä½é †åºï¼Œç¼ºæ¬„å‰‡é‡æŠ“
    if cache_hit_path is not None:
        logging.info("å¿«å–å‘½ä¸­ï¼š%s", cache_hit_path.resolve())
        df = pd.read_csv(cache_hit_path)
        missing = [c for c in CACHE_COLUMNS if c not in df.columns]
        if not missing:
            df = df[CACHE_COLUMNS]
            if cache_hit_path != target_path and not target_path.exists():
                try:
                    compression = "gzip" if target_path.suffix == ".gz" else None
                    df.to_csv(target_path, index=False, encoding="utf-8-sig", compression=compression)
                    logging.info("å·²åŒæ­¥å»ºç«‹å¿«å–ï¼š%s", target_path.resolve())
                except Exception as exc:
                    logging.warning("åŒæ­¥å¿«å–å¤±æ•—ï¼š%s", exc)
            return df
        logging.warning("å¿«å–æ¬„ä½ç¼ºå¤±ï¼š%sï¼›å¿½ç•¥å¿«å–æ”¹ç‚ºé‡æŠ“ã€‚", missing)

    if from_cache_only:
        logging.info("from-cache-only å•Ÿç”¨ï¼Œæœªæ‰¾åˆ°å¿«å–ï¼Œç•¥éä¸‹è¼‰ï¼š%s", day)
        return None

    params = {"response": "json", "date": day.strftime("%Y%m%d"), "type": "ALL"}
    logging.info("ä¸‹è¼‰ %s ...", day.isoformat())
    try:
        r = session.get(TWSE_MI_INDEX, params=params)
        r.raise_for_status()
        data = r.json()
    except Exception as e:
        logging.warning("ä¸‹è¼‰å¤±æ•— %sï¼š%s", day, e)
        return None

    if data.get("stat") != "OK":
        logging.info("è©²æ—¥ç„¡äº¤æ˜“è³‡æ–™æˆ–å°šæœªé–‹ç›¤ï¼š%s", day)
        return None

    df = None
    for table in data.get("tables", []):
        fields = table.get("fields") or []
        rows = table.get("data") or []
        if fields[:2] == ["è­‰åˆ¸ä»£è™Ÿ", "è­‰åˆ¸åç¨±"]:
            df = pd.DataFrame(rows, columns=fields)
            break

    if df is None or df.empty:
        logging.warning("æœªæ‰¾åˆ°è­‰åˆ¸è¡¨æ ¼ï¼š%sï¼›keys=%s", day, list(data.keys()))
        return None

    rename_map = {
        "è­‰åˆ¸ä»£è™Ÿ": "ä»£è™Ÿ",
        "è­‰åˆ¸åç¨±": "åç¨±",
        "é–‹ç›¤åƒ¹": "é–‹ç›¤",
        "æœ€é«˜åƒ¹": "æœ€é«˜",
        "æœ€ä½åƒ¹": "æœ€ä½",
        "æ”¶ç›¤åƒ¹": "æ”¶ç›¤",
        "æˆäº¤é‡‘é¡": "æˆäº¤é‡‘é¡",  # å–®ä½å¸¸ç‚ºåƒå…ƒ
    }
    df = df.rename(columns=rename_map, errors="ignore")

    # åƒ…ä¿ç•™å››ç¢¼è‚¡ç¥¨ï¼ˆæ’é™¤æ¬Šè­‰/å¯è½‰å‚µç­‰ï¼‰ï¼Œå¯ä¾éœ€æ±‚èª¿æ•´
    df = df[df["ä»£è™Ÿ"].astype(str).str.match(r"^[1-9]\d{3}$", na=False)]

    for col in ["é–‹ç›¤", "æœ€é«˜", "æœ€ä½", "æ”¶ç›¤", "æˆäº¤é‡‘é¡"]:
        df[col] = (
            df[col]
            .astype(str)
            .str.replace(",", "", regex=False)
            .replace(["--", "", "nan", "None"], pd.NA)
        )
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df["æ—¥æœŸ"] = day.strftime("%Y-%m-%d")
    df["è³‡æ–™ä¾†æº"] = "TWSE"
    df["ä¸‹è¼‰æ™‚é–“"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    df = df[CACHE_COLUMNS]
    df = df.dropna(subset=["æ”¶ç›¤"])

    # å¿«å–
    compression = "gzip" if target_path.suffix == ".gz" else None
    df.to_csv(target_path, index=False, encoding="utf-8-sig", compression=compression)
    logging.info("å·²å¿«å–ï¼š%sï¼ˆ%d ç­†ï¼‰", target_path.resolve(), len(df))
    return df

# ---------------------------
# ä¸»æµç¨‹
# ---------------------------

def daterange_by_args(args) -> Tuple[date, date, int]:
    """æ ¹æ“š --from/--to/--days æ¨å°æ—¥æœŸç¯„åœèˆ‡ç›®æ¨™ã€Œäº¤æ˜“æ—¥æ•¸ã€"""
    tz_today = date.today()
    if args.date_from and args.date_to:
        start = datetime.strptime(args.date_from, "%Y-%m-%d").date()
        end = datetime.strptime(args.date_to, "%Y-%m-%d").date()
        n_days = None
    elif args.days:
        end = tz_today
        start = end - timedelta(days=max(args.days * 2, args.days + 30))  # çµ¦å¯¬ä¸€é»çš„åŸå§‹ç¯„åœï¼Œä¹‹å¾Œå†ä»¥äº¤æ˜“æ—¥è£åˆ‡
        n_days = args.days
    else:
        # é è¨­æŠ“æœ€è¿‘ 60 å€‹äº¤æ˜“æ—¥
        end = tz_today
        start = end - timedelta(days=180)
        n_days = 60
    return start, end, n_days or 0

def as_rows(df: pd.DataFrame) -> Iterable[Tuple]:
    for r in df.itertuples(index=False):
        volume = getattr(r, "æˆäº¤é‡‘é¡")
        volume_value = None if pd.isna(volume) else int(float(volume))
        yield (
            r.æ—¥æœŸ, r.ä»£è™Ÿ, r.åç¨±, r.é–‹ç›¤, r.æœ€é«˜, r.æœ€ä½, r.æ”¶ç›¤, volume_value, r.è³‡æ–™ä¾†æº, r.ä¸‹è¼‰æ™‚é–“
        )

def run(args) -> None:
    setup_logging(args.log_level, Path(args.data_dir))
    session = make_session(max_retries=args.max_retries, backoff=0.6, timeout=12, verify=not args.no_verify)

    data_dir = Path(args.data_dir)
    db_path = Path(args.db_path) if args.db_path else DEFAULT_DB_PATH
    init_db(db_path)

    start, end, target_trade_days = daterange_by_args(args)
    all_days = build_trading_days(session, start, end, data_dir, refresh_calendar=args.refresh_calendar)

    # è‹¥æŒ‡å®š --daysï¼Œåªå–æœ€å¾Œ N å€‹äº¤æ˜“æ—¥
    if target_trade_days:
        all_days = all_days[-target_trade_days:]
        logging.info("å–æœ€å¾Œ %d å€‹äº¤æ˜“æ—¥ï¼š%s ~ %s", target_trade_days, all_days[0], all_days[-1])

    total_inserted = 0
    collected_rows: List[Tuple] = []
    consecutive_failures = 0
    halt_on_fail = max(args.halt_on_fail, 0)

    # (C) ä¸­æ–·ä¿è­·ï¼šç¢ºä¿æ®˜é¤˜ flush
    try:
        for i, d in enumerate(all_days, 1):
            df = fetch_one_day(
                session,
                d,
                data_dir,
                force=args.force,
                cache_format=args.out_format,
                from_cache_only=args.from_cache_only,
            )
            should_break = False
            if df is not None and not df.empty:
                collected_rows.extend(as_rows(df))
                logging.info("é€²åº¦ï¼š%d/%d äº¤æ˜“æ—¥ï¼›ç›®å‰ç´¯ç© %d ç­†", i, len(all_days), len(collected_rows))
                consecutive_failures = 0
            else:
                consecutive_failures += 1
                logging.warning("äº¤æ˜“æ—¥ %s è³‡æ–™å–å¾—å¤±æ•—ï¼ˆé€£çºŒ %d æ¬¡ï¼‰", d, consecutive_failures)
                if halt_on_fail and consecutive_failures >= halt_on_fail:
                    logging.error("é€£çºŒå¤±æ•—é” %d æ¬¡ï¼Œææ—©åœæ­¢ã€‚", consecutive_failures)
                    should_break = True
            if should_break:
                break
            time.sleep(args.sleep)

            # ä»¥æ‰¹é‡å¤§å°å¯«å…¥ï¼Œé¿å…è¨˜æ†¶é«”æš´è¡
            if len(collected_rows) >= args.batch_size:
                inserted = bulk_upsert(db_path, collected_rows)
                total_inserted += inserted
                logging.info("æ‰¹æ¬¡å…¥åº« %d ç­†ï¼ˆç¸½è¨ˆ %dï¼‰", inserted, total_inserted)
                collected_rows.clear()
    finally:
        if collected_rows:
            inserted = bulk_upsert(db_path, collected_rows)
            total_inserted += inserted
            logging.info("æ”¶å°¾å…¥åº« %d ç­†ï¼ˆç¸½è¨ˆ %dï¼‰", inserted, total_inserted)

    logging.info("å®Œæˆã€‚DB è·¯å¾‘ï¼š%sï¼›è³‡æ–™å¤¾ï¼š%s", db_path.resolve(), data_dir.resolve())
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼Œç¸½å…±è™•ç† {total_inserted} ç­†è³‡æ–™")
    print(f"DBï¼š{db_path.resolve()}")
    print(f"å¿«å–è³‡æ–™å¤¾ï¼š{data_dir.resolve()}")

# ---------------------------
# CLI
# ---------------------------

def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description="TWSE æ—¥ç·šæŠ“å–å™¨")
    grp_range = p.add_mutually_exclusive_group()
    grp_range.add_argument("--days", type=int, help="æŠ“æœ€è¿‘ N å€‹ã€äº¤æ˜“æ—¥ã€")
    p.add_argument("--from", dest="date_from", help="èµ·å§‹æ—¥æœŸ YYYY-MM-DDï¼ˆèˆ‡ --to æ­é…ï¼‰")
    p.add_argument("--to", dest="date_to", help="çµæŸæ—¥æœŸ YYYY-MM-DDï¼ˆèˆ‡ --from æ­é…ï¼‰")

    p.add_argument("--sleep", type=float, default=0.2, help="æ¯æ—¥ä¸‹è¼‰é–“éš”ç§’æ•¸ï¼ˆé¿å…éå¿«ï¼‰")
    p.add_argument("--max-retries", type=int, default=3, help="HTTP ä¸‹è¼‰æœ€å¤§é‡è©¦æ¬¡æ•¸")
    p.add_argument("--batch-size", type=int, default=5000, help="DB æ‰¹æ¬¡å¯«å…¥ç­†æ•¸")
    p.add_argument("--force", action="store_true", help="ç„¡è¦–å¿«å–ï¼Œå¼·åˆ¶é‡æŠ“ä¸¦è¦†å¯« CSV")
    p.add_argument("--log-level", default="INFO", help="DEBUG / INFO / WARNING / ERROR")
    p.add_argument("--data-dir", default=str(DEFAULT_DATA_DIR), help="è³‡æ–™å¿«å–è³‡æ–™å¤¾")
    p.add_argument("--out-format", choices=["csv", "csv.gz"], default="csv", help="å¿«å–è¼¸å‡ºæ ¼å¼ï¼ˆcsv æˆ– csv.gzï¼‰")
    p.add_argument("--from-cache-only", action="store_true", help="åƒ…ä½¿ç”¨æ—¢æœ‰å¿«å–ï¼Œä¸åŸ·è¡Œç¶²è·¯ä¸‹è¼‰")
    p.add_argument("--db-path", default=str(DEFAULT_DB_PATH), help="SQLite è·¯å¾‘")
    p.add_argument("--refresh-calendar", action="store_true", help="å¿½ç•¥å¿«å–ï¼Œå¼·åˆ¶é‡æ–°æŠ“å–äº¤æ˜“æ—¥è¡Œäº‹æ›†")
    p.add_argument("--halt-on-fail", type=int, default=20, help="é€£çºŒæŠ“å–å¤±æ•—é”æŒ‡å®šæ¬¡æ•¸å¾Œæå‰åœæ­¢ï¼ˆ0 è¡¨ç¤ºä¸åœï¼‰")
    p.add_argument("--no-verify", action="store_true", help="åœç”¨ SSL æ†‘è­‰é©—è­‰ (æ…ç”¨)")
    return p.parse_args(argv)

if __name__ == "__main__":
    args = parse_args()
    run(args)
